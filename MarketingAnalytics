options(java.parameters = "-Xmx2000m")

install_github("pablobarbera/Rfacebook/Rfacebook")
install.packages("tm")
install.packages("RSentiment")
suppressWarnings(library(Rfacebook))
suppressWarnings(library(tm))
suppressWarnings(library(RSentiment))

install.packages("reshape2")
install.packages("bsts")
install.packages("devtools")
install.packages("dplyr")
install.packages("ggplot2")
install.packages("forecast")
install.packages("reshape")
install.packages("CausalImpact")

install.packages("foreach")
install.packages("caret")
install_github("google/CausalImpact")
install.packages("lubridate")
install.packages("pls")

install.packages("Performance Analytics")
suppressWarnings(library(Hmisc))
suppressWarnings(library(devtools))
suppressWarnings(library(lubridate))
suppressWarnings(library(bsts))
suppressWarnings(library(dplyr))
suppressWarnings(library(ggplot2))
suppressWarnings(library(forecast))
suppressWarnings(library(reshape))
suppressWarnings(library(reshape2))
suppressWarnings(library(CausalImpact))
suppressWarnings(library(foreach))
suppressWarnings(library(caret))
suppressWarnings(library(pls))

##########################################################################
#Google Trends data aggregation for related search terms

#Paste associated keywords from google trends with .csv
#Read in the csv downloaded from google trends with keyword search history index values
#Return dataframe with the keyword's search history (date, index)

ReadFiles = function(FileName) {
  i = sprintf("%s%s", FileName, ".csv", sep = "")
  df = read.csv(i, header = FALSE, stringsAsFactors = FALSE, skip = 2)
  return(df)
}

#List of csv filenames created for each of the top 13 keywords returned when searching 'ClientName'
Keywords = c("ProductGoogleTrends","CompetitorBGoogleTrends","ProductGoogleTrends",
             "ClientName.comGoogleTrends","ClientNameCommercialGoogleTrends","ClientNameGoogleTrends",
             "ClientNameFaceOfTheBrandGoogleTrends","ClientNameFaceOfTheBrandProductGoogleTrends",
             "ClientNameCompetitorAGoogleTrends","ClientNameReviewGoogleTrends",
             "ClientNameProductGoogleTrends","ClientNameSubProductProductGoogleTrends",
             "ClientNameSubProductGoogleTrends")
             
#Generate a list of dataframes, each dataframe corresponds to a keyword csv (2 columns, date & index value)
Keywordlist = list()
Keywordlist = lapply(Keywords, function(x) ReadFiles(x))

#Draw index value columns from the list of dataframes; since all dates are identical for each keyword, only draw out date column for the 1st dataframe
KeywordIndexes = Keywordlist[[1]][1]
for(i in 1:length(Keywordlist)) KeywordIndexes = cbind(KeywordIndexes, Keywordlist[[i]][2])

#narrow the observations to include only the weeks covering client provided data
KeywordIndexes = KeywordIndexes[-(2:4),]; KeywordIndexes = KeywordIndexes[-(107:115),];

#colnames for the variables are the search terms
KeywordNames = c()
for(i in 1:length(KeywordIndexes)){
  KeywordNames[i] = regmatches(KeywordIndexes[1,i], 
                             regexpr("[A-Za-z \\.]*",KeywordIndexes[1,i]))
}
KeywordNames = gsub(" ", ".", KeywordNames)
KeywordNames[KeywordNames=="Week"] = "Date"

colnames(KeywordIndexes) = KeywordNames
KeywordIndexes  = KeywordIndexes[-1,]

KeywordIndexes[,"Date"] = as.Date(KeywordIndexes[,"Date"], format = "%m/%d/%Y")

#Write data out into a consolidated csv so we can append it to our dataset later
#write.csv(KeywordIndexes, file = "ClientNameKeyWordsGoogleTrends.csv")

#if this needs to be run again, execute the following
#file.remove("ClientNameKeyWordsGoogleTrends.csv")

#end of google trends script
########################################################################

#######################################################################
#facebook scraping instructions

#for FB package, note that you have to get a new token if you log out of a session
#token: temporary access token created at https://developers.facebook.com/tools/explorer or the OAuth token created with fbOAuth.

#scrape ClientName public page for posts that map to dataset; note reactions are not supported
fableticsFBdata =  getPage("ClientName", token, n = 90000, since = 'xxxx/xx/xx', until = 'xxxx/xx/xx', feed = TRUE, reactions = FALSE, verbose = FALSE)

#write.csv(ClientNameFBdata, file = "ClientNameFBData.csv")

#ClientNameFBdata = read.csv("ClientNameFBData.csv", header = TRUE)

###############################################################

##########################################################
#ClientName FB comments sentiment analysis

#draws out comment messages
ClientNameComments = as.matrix(ClientNameFBdata[,"message"])

#takes punctuation, extra whitespace, trailing space out
ClientNameText = gsub("[^A-Za-z' .-]", "", ClientNameComments)
ClientNameText = gsub("(\\.\\.\\.|-|\\.)"," ", ClientNameText)
ClientNameText = gsub("\\s+"," ", ClientNameText)
ClientNameText = gsub(" $","", ClientNameText)

#fields that contain no terms must be replaced
ClientNameText[ClientNameText == "" | is.na(ClientNameText),] = "NoTerms"

#turn messages into a corpus
ClientNameUniText.corp = Corpus(VectorSource(ClientNameText))

#convert the corpus to lowercase to facilitate sentiment analysis
ClientNameUniText.corp = tm_map(ClientNameUniText.corp,content_transformer(tolower))

#remove stopwords
ClientNameUniText.corp = tm_map(ClientNameUniText.corp,removeWords,stopwords("english"))

#convert to dataframe
ClientNameUniText.corp = data.frame(ClientNameComments = get("content",ClientNameUniText.corp),stringsAsFactors = FALSE)

#remove excess spacing
ClientNameUniText.corp = sapply(ClientNameUniText.corp, function(x) gsub("\\s+", " ",x))

#split stop-word-cleaned free text into elements to identify custom valence terms
FreeTextFreqTable = function(CorporaToClean) {

  #free text has to be in matrix form
  CorporaToClean = as.matrix(CorporaToClean)

  #separate each row string of text at every instance of white space
  Sep_at_Spaces = strsplit(CorporaToClean, " ")
  
  #remove structure to identify high frequency terms
  Terms = unlist(Sep_at_Spaces)
  
  #generate a frequency table
  Freq_of_terms = table(Terms)

  #put them in a user friendly matrix, order them for selection
  Freq_of_terms_tbl = as.matrix(Freq_of_terms)
  Freq_of_terms_tbl = as.matrix(Freq_of_terms_tbl[order(-Freq_of_terms_tbl),])

}

#get the frequency table from the message field of scrapped FB data
ClientNameTextTbl = FreeTextFreqTable(ClientNameUniText.corp)

#write.csv(ClientNameTextTbl, "ClientName.comment.terms.csv")

#use the .csv file for custom lists of valence terms, save as Pos-Neg

PosNeg = read.csv("ClientName.comment.terms.Pos-Neg.csv", header = TRUE, stringsAsFactors = FALSE)

#extract the custom vectors of sentiment terms
NegTerms = PosNeg[PosNeg[,1] != "",1]

PosTerms = PosNeg[PosNeg[,2] != " " & PosNeg[,2] != "",2] 

#run sentiment analysis on each comment; note that the model is preliminary
ClientNameSent = sapply(ClientNameText, function(x) calculate_custom_sentiment(x, PosTerms, NegTerms)[2])

#extract the sentiment assessments from the output, as a character vector
ClientNameMsg = c(); for(i in 1:length(ClientNameSent)) ClientNameMsg[i] = as.character(levels(ClientNameSent[[i]]))

#tune the sentiment calculator by changing terms in the custom polarity dictionary
table(ClientNameMsg)

#combine with original data for comparison
ClientNameFBdataSen = cbind(ClientNameFBdata[,1:3],"sentiment" = ClientNameMsg,
                           ClientNameFBdata[,4:ncol(ClientNameFBdata)])

#remove ClientName sourced posts for an assessment of external party sentiment
ClientNameFBdataExClientName = ClientNameFBdataSen[ClientNameFBdataSen[,"from_name"] != "ClientName",]

table(ClientNameFBdataExClientName[,"sentiment"])

#convert time stamps to dates
ClientNameFBdataExClientName[,"created_time"] = as.Date(ClientNameFBdataExClientName[,"created_time"])

#generate the dates that will encompass a weekly bin of sentiment values
FstDt = KeywordIndexes[,"Date"]
SecDt = KeywordIndexes[,"Date"] + 7

ClientNameFBdataExClientName[,"sentiment"] = as.character(ClientNameFBdataExClientName[,"sentiment"])

#generate a list of 1 week bins
ClientNameSentlist = list()
for(i in 1:length(FstDt)){
  ClientNameSentlist[[i]] = ClientNameFBdataExClientName[which(ClientNameFBdataExClientName[,"created_time"] >= FstDt[i] & 
  ClientNameFBdataExClientName[,"created_time"] < SecDt[i]),"sentiment"]
}

#convert character sentiment to numeric
CharV = c("Very Negative","Negative","Neutral","Positive","Very Positive")
NumbV = c("-2","-1","0","1","2")

for(i in 1:length(CharV)) ClientNameSentlist = lapply(ClientNameSentlist, function(x) gsub(paste("^",CharV[i],"$", sep = ""), NumbV[i], x))

ClientNameSentlist = lapply(ClientNameSentlist, function(x) as.numeric(x))

#sum the numeric values to determine each week's sentiment index
ClientNameWkSent = c()
for(i in 1:length(ClientNameSentlist)) ClientNameWkSent[i] = sum(as.vector(ClientNameSentlist[[i]]))

#attach sentiment indexes to google trends data
KeywordIndexes$sentiment = ClientNameWkSent

#find the proportion of all messages corresponding to each week
Prop = unlist(lapply(ClientNameSentlist, function(x) length(x)))
Prop = 100 * Prop / length(unlist(ClientNameSentlist))

#weight weekly sentiment values with comment count proportions
#reduces weekly sentiment inflation of a few very pos/neg comments
KeywordIndexes$sentiment = KeywordIndexes$sentiment * Prop

#end of sentiment analysis
##############################################################################
#begin data integration

#the data
st = read.csv("St.csv", header = TRUE)
st = st[,-which(names(st) %in% c("X","CS"))] #remove unary vars

#remove punctuation from character variables
st$G = as.factor(gsub("\\.", "", st$G))
st$Pr = as.factor(gsub("\\$", "", st$Pr))

#as per guidance, we can treat all st FB as PdS
#to make this work using factors instead of character strings
#add PdS to the factors of the variable
levels(st$SuC) = c(levels(st$SuC), "PdS")

#find rows where SuC is FB and replace it with PdS
st[st$SuC == "FB","SuC"] = "PdS"

#remove FB from the variable levels, this resets the factor levels to only those present
st$SuC = factor(st$SuC)

#merge the Ch and SuC columns for both dfs
st$ChSuCh = as.factor(paste(st$Ch, st$SuC))

#update the list of factor names
levels(st$ChSuCh) = c(levels(st$ChSuCh), "PdS", "T")

#replace the ones you don't want
st[st$ChSuCh == "PdS PdS","ChSuCh"] = "PdS"
st[st$ChSuCh == "T T","ChSuCh"] = "T"

#remove the ones no longer used
st$ChSuCh = factor(st$ChSuCh)

#remove unnecessary original variables
st = st[,-which(names(st) %in% c("Ch","SuC"))]

#re data
re = read.csv("Re.csv", header = TRUE)
re = re[,-which(names(re) %in% c("SL"))]

#remove punctuation from G, Pr
re$G = as.factor(gsub("\\.", "", re$G))
re$Pr = as.factor(gsub("\\$", "", re$Pr))
re$Pr = as.factor(gsub("\\%", " percent", re$Pr))

#as per guidance, remove RS from PoS
re = re[-which(re$PoS == "RS"),]

#same factor conversion for re data
re$ChSuCh = as.factor(paste(re$Ch, re$SuC))

#update
levels(re$ChSuCh) = c(levels(re$ChSuCh), "Af",
                                       "FR", "OD", "PdS", "SD")
#replace
re[re$ChSuCh == "Af Af","ChSuCh"] = "Af"
re[re$ChSuCh == "FR FR","ChSuCh"] = "FR"
re[re$ChSuCh == "OD OD","ChSuCh"] = "OD"
re[re$ChSuCh == "PdS PdS","ChSuCh"] = "PdS"
re[re$ChSuCh == "SD SD","ChSuCh"] = "SD"

#remove the replaced
re$ChSuCh = factor(re$ChSuCh)

#remove originals
re = re[,-which(names(re) %in% c("Ch","SuC", "PoS"))]

#################################################################
#pivot tables

#re: pivot table of ACT by date and all possible combinations of categories
re.pivot.act = cast(re, Date ~ ChSuCh + G + CS + Pr, value = "V.A")

#re: do the same for AC costs
re.pivot.cst = cast(re, Date ~ ChSuCh + G + CS + Pr, value = "AC")

#re: confirm that all the column names are equivalent for AC and cost
length(which(names(re.pivot.act) == names(re.pivot.cst)))

#re: append an ACT designator to the col names for AC
for (i in 1:length(re.pivot.act)) {
  colnames(re.pivot.act)[i] = paste(colnames(re.pivot.act)[i], ".ACT", sep = "")
}

#re: append designator for costs
for (i in 1:length(re.pivot.cst)) {
  colnames(re.pivot.cst)[i] = paste(colnames(re.pivot.cst)[i], ".ACC", sep = "")
}

#re: replace NAs with 0s
re.pivot.act[is.na(re.pivot.act)] = 0
re.pivot.cst[is.na(re.pivot.cst)] = 0

#re: find the aggregates for a given week, append them to the pivot table
re.pivot.act$VACT = rowSums(re.pivot.act[,2:length(re.pivot.act)])
re.pivot.cst$ACC = rowSums(re.pivot.cst[,2:length(re.pivot.cst)])

#st: pivot table of IM by date and all possible combinations of categories
st.pivot.imp = cast(st, Date ~ ChSuCh + G + Pr, value = "IM")

#now the same for MS
st.pivot.spd = cast(st, Date ~ ChSuCh + G + Pr, value = "MS")

#st: confimn that all the column names are equivalent for IM and spend
length(which(names(st.pivot.imp) == names(st.pivot.spd)))

#st: append an ACT designator to the col names for IM
for (i in 1:length(st.pivot.imp)) {
  colnames(st.pivot.imp)[i] = paste(colnames(st.pivot.imp)[i], ".IM", sep = "")
}

#st: append designator for spd
for (i in 1:length(st.pivot.spd)) {
  colnames(st.pivot.spd)[i] = paste(colnames(st.pivot.spd)[i], ".MSP", sep = "")
}

#st: replace NAs with 0s
st.pivot.imp[is.na(st.pivot.imp)] = 0
st.pivot.spd[is.na(st.pivot.spd)] = 0

#re: find the aggregates for a given week, append them to the pivot table
st.pivot.imp$IM = rowSums(st.pivot.imp[,2:length(st.pivot.imp)])
st.pivot.spd$MS = rowSums(st.pivot.spd[,2:length(st.pivot.spd)])

#integrate datasets for modeling
ClientName.pivot.table = cbind(re.pivot.act, re.pivot.cst[,2:length(re.pivot.cst)], st.pivot.imp[,2:length(st.pivot.imp)], st.pivot.spd[,2:length(st.pivot.spd)])

#how many VACT during this period?
sum(re.pivot.act$VACT)
#how much did it cost them to acquire ACT?
sum(re.pivot.cst$ACC)
#how many IM did they generate?
sum(st.pivot.imp$IM)
#marketing spend to generate IM?
sum(st.pivot.spd$MS)

#find sums for each combination of variables (C, ACT, IM, MSP)
PivotColSums = function(CNPT){
  
  Df = colSums(CNPT[2:length(CNPT)])
  Df = c(nrow(CNPT),Df) #to complete dataset add count of total dates
  Df = data.frame(Df) #convert to dataframe
  Df
}

AllColSums = PivotColSums(ClientName.pivot.table)

#how many columns add to zero?
length(which(AllColSums == 0))

#find unvary variables, use this vector as an index for location in pivot table
FindAllZeros = which(AllColSums == 0)

#get variable names of unaries, by index values
ZeroRowNames = colnames(ClientName.pivot.table)[FindAllZeros]

#remove all zero columns from the pivot table
ClientName.pivot.table = ClientName.pivot.table[,-which(names(ClientName.pivot.table) %in% ZeroRowNames)]

#rename date variable
colnames(ClientName.pivot.table)[1] = "Date"

#rerun to remove all zero columns from AllColSums
AllColSums = PivotColSums(ClientName.pivot.table)

#now build a correlation matrix for all combinations
All.Correlations = cor(ClientName.pivot.table[,2:length(ClientName.pivot.table)])

#end of pivot table generation, correlation matrices, column sums for each factor combo
###############################################################################

##############################################################################
##subset the pivot table of time series objects by any desired variable

#pull the factor levels as terms to match in the pivot table column names
#pull AC and V.A from these
reG = levels(re$G)
reChaSub = levels(re$ChSuCh)
reCSG = levels(re$CS)
rePr = levels(re$Pr)
#remove the $ symbol so grep() can be used
rePr = gsub("\\$", "", rePr)
rePr = gsub("\\%", " percent", rePr)

#pull IM and MS from these
stGG = levels(st$G)
stChanSub = levels(st$ChSuCh)
stPr = levels(st$Pr)

#identify columns with target variable and factor
#a numeric list of columns where the level (subset criteria) is found in the pivot table
#e.g. Subsetcriteria = "CityA" or "PromoA", finds every col name that contains that info
GrepSubset = function(SubsetCriteria) {
  grep(SubsetCriteria, names(ClientName.pivot.table))
}

#rather than do it for each variable and factor individually, sapply the above function
#to the dataset, generate a list of index values for column names that contain each level
ListofDFsGenerator = function(LevelsToInput) {
  ListToOutput = list()
  for (i in 1:length(LevelsToInput)) {
    ListToOutput[[i]] = GrepSubset(LevelsToInput[i])
  }
  return(ListToOutput)
}

#execute the ListGenerator, which executes GrepSubset for all levels in each variable
re.GS = ListofDFsGenerator(reGG)     #list of lists, each list greps a variable's columns
re.SubChs = ListofDFsGenerator(reChaSub)
re.CSSG = ListofDFsGenerator(reCSG)
re.Prs = ListofDFsGenerator(rePr)
st.GS = ListofDFsGenerator(stGG)
st.SubChs = ListofDFsGenerator(stChanSub)
st.Prs = ListofDFsGenerator(stPr)            #all var levels captured & grepd

#a function that generates subsets from pivot table that correspond to a G, segment, Pr, or subCh
Aggregates = function(VN,VC) {

  #use the location values to subset a pivot table by a desired variable or factor
  VN = as.vector(VC)

  #see what is going on in a given city
  VD = ClientName.pivot.table[,VN]
  VD = cbind(ClientName.pivot.table[1], VD)

  #find the subset in the data that corresponds to VACT, get the total ACT for each date
  #cbind to the dataframe
  VD$ACT = rowSums(VD[, grep("ACT", colnames(VD))])

  #do the same for IM, MS, and ACC
  VD$IM = rowSums(VD[, grep("IM", colnames(VD))])
  VD$MS = rowSums(VD[, grep("MSP", colnames(VD))])
  VD$ACC = rowSums(VD[, grep("ACC", colnames(VD))])
  return(VD)
}

#run the Aggregator on every variable level, group pivot table columns
#appends counts for each numeric metric in the dataset (e.g. ACT, IM)
#inputs are the variable levels, e.g. st.G[1] = CityA
#grep index values for each level in st.G (columns where a level is found)
AggregateGenerator = function(InputLevels, InputGreps) {
  Levels = list()
  for (i in 1:length(InputLevels)) {
    Levels[[i]] = Aggregates(InputLevels[i],InputGreps[[i]])
  }
  return(Levels)
}

st.DMA = AggregateGenerator(stGG, st.GS)
#name list elements their corresponding G
names(st.DMA) = stGG

st.Chs = AggregateGenerator(stChanSub, st.SubChs)
names(st.Chs) = st.ChanSub

st.Pr = AggregateGenerator(stPr, st.Prs)
names(st.Pr) = stPr

re.DMA = AggregateGenerator(reG, re.GS)
names(re.DMA) = reG

re.Chs = AggregateGenerator(reChaSub, re.SubChs)
names(re.Chs) = reChaSub

re.CSSG = AggregateGenerator(reCSG, re.CSSG)
names(re.CSSG) = reCSG

re.Pr = AggregateGenerator(rePr, re.Prs)
names(re.Pr) = rePr

#put aggregates across all Chs, Prs, etc. into a single df by metric (IM)
#then do the same for other metrics: ACC, IM, ACT, MS
#st.DMA[["CityA"]]["IM"]; st.DMA[["CityA"]]["MS"]; #etc.

#bind all the city IM into a single df
IMByG = cbind(st.DMA[["CityA"]]["IM"],
                               st.DMA[["CityB"]]["IM"],
                               st.DMA[["CityC"]]["IM"],
                               st.DMA[["CityD"]]["IM"],
                               st.DMA[["CityE"]]["IM"],
                               st.DMA[["CityF"]]["IM"],
                               st.DMA[["CityG"]]["IM"],
                               st.DMA[["CityH"]]["IM"],
                               st.DMA[["CityI"]]["IM"],
                               st.DMA[["CityJ"]]["IM"],
                               st.DMA[["CityK"]]["IM"],
                               st.DMA[["CityL"]]["IM"],
                               st.DMA[["CityM"]]["IM"],
                               st.DMA[["CityN"]]["IM"],
                               st.DMA[["CityO"]]["IM"],
                               st.DMA[["CityP"]]["IM"],
                               st.DMA[["CityQ"]]["IM"],
                               st.DMA[["CityR"]]["IM"],
                               st.DMA[["CityS"]]["IM"],
                               st.DMA[["CityT"]]["IM"],
                               st.DMA[["CityU"]]["IM"],
                               st.DMA[["CityV"]]["IM"])

#assign them G names
names(IMByG) = levels(st$G)

#IM by DMA
#count matches IM in st data
sum(colSums(IMByG))
sum(st$IM)

#consolidate by factors
Cons.fun = function(Data,Subset,Factor){
  
  MSByX = Data[[Subset]][Factor]
  foreach( x = 2:length(Data), .combine = cbind) %do% {
   MSByX[x] = Data[[x]][Factor] 
  }
  MSByX
}
#MS by DMA
MSByG = Cons.fun(st.DMA,"CityA","MS")
#assign them G names
names(MSByG) = levels(st$G)

#run the same code sum() comparison for each aggregation to ensure integrity
sum(colSums(MSByG))
sum(st$MS)

#note: wherever you get a variable pulling from the wrong dataset, remove it
#e.g. re.GS$IM is wrong, only pull st.GS$IM
#similarly, st.SubChs$V.A is wrong, should only be re.SubChs$V.A

#generate the same time series matrices for st subChs
IMBySubCh = Cons.fun(st.Chs,"OD Content","IM")
names(IMBySubCh) = levels(st$ChSuCh)

#IM by subCh
sum(colSums(IMBySubCh))
sum(st$IM)

#MS by SubCh
MSBySubCh = Cons.fun(st.Chs,"OD Content","MS")
names(MSBySubCh) = levels(st$ChSuCh)

#run the same code sum() comparison for each aggregation to ensure integiry
sum(colSums(MSBySubCh))
sum(st$MS)

#for stPr
IMByPr = Cons.fun(st.Pr,"PR1","IM")
names(IMByPr) = levels(st$Pr)

#IM by Pr
sum(colSums(IMByPr))
sum(st$IM)

MSByPr = Cons.fun(st.Pr,"PR1","MS")
names(MSByPr) = levels(st$Pr)

#ensure integrity
sum(colSums(MSByPr))
sum(st$MS)

#for re.DMAs
ACTByG = Cons.fun(re.DMA,"CityA","ACT")
names(ACTByG) = levels(re$G)

#ACT by G
sum(colSums(ACTByG))
sum(re$V.A)

ACCByG = Cons.fun(re.DMA,"CityA","ACC")
names(ACCByG) = levels(re$G)

#ACC by G
sum(colSums(ACCByG))
sum(re$AC)

#for re.subChs
ACTBySubCh = Cons.fun(re.Chs,"PSB","ACT")
names(ACTBySubCh) = levels(re$ChSuCh)

#ACT by G
sum(colSums(ACTBySubCh))
sum(re$V.A)

ACCBySubCh = Cons.fun(re.Chs,"PSB","ACC")
names(ACCbySubCh) = levels(re$ChSuCh)

#ACC by G
sum(colSums(ACCbySubCh))
sum(re$AC)

#for re.CS
ACTByCSG = Cons.fun(re.CSSG,"AL","ACT")
names(ACTByCSG) = levels(re$CS)

#ACT by G
sum(colSums(ACTByCSG))
sum(re$V.A)

ACCByCSG = Cons.fun(re.CSSG,"AL","ACC")
names(ACCbyCSG) = levels(re$CS)

#ACC by G
sum(colSums(ACCbyCSG))
sum(re$AC)

#for re.Prs
ACTByPr = Cons.fun(re.Pr,"PR1","ACT")
names(ACTByPr) = levels(re$Pr)

#ACT by G
sum(colSums(ACTByPr))
sum(re$V.A)

ACCByPr = Cons.fun(re.Pr,"PR1","ACC")
names(ACCbyPr) = levels(re$Pr)

#ACC by G
sum(colSums(ACCbyPr))
sum(re$AC)

#use these to get the column names from the list of lists
#names(re.Chs[[1]])[130]
#levels(re$Pr)

#metric designator to the col names for each df, for aggregation of all dfs
AppendDesignator = function(dfName,Designator) {
  for (i in 1:length(dfName)) {
    colnames(dfName)[i] = paste(colnames(dfName)[i], Designator, sep = "")
  }
  return(dfName)
}
IMByG = AppendDesignator(IMByG,".IM")
IMBySubCh = AppendDesignator(IMBySubCh,".IM")
IMByPr = AppendDesignator(IMByPr,".IM")

MSByG = AppendDesignator(MSByG,".MS")
MSBySubCh = AppendDesignator(MSBySubCh,".MS")
MSByPr = AppendDesignator(MSByPr,".MS")

ACTByG = AppendDesignator(ACTByG,".ACT")
ACTBySubCh = AppendDesignator(ACTBySubCh,".ACT")
ACTByCSG = AppendDesignator(ACTByCSG,".ACT")
ACTByPr = AppendDesignator(ACTByPr,".ACT")

ACCbyG = AppendDesignator(ACCbyG,".ACC")
ACCbySubCh = AppendDesignator(ACCbySubCh,".ACC")
ACCbyCSG = AppendDesignator(ACCbyCSG,".ACC")
ACCbyPr = AppendDesignator(ACCbyPr,".ACC")

#the complete data set ready for conversion to time series
IMAsTS = cbind.data.frame("Date" = ClientName.pivot.table[,"Date"], IMByG,
                                   IMBySubCh, IMByPr)

MSAsTS = cbind.data.frame("Date" = ClientName.pivot.table[,"Date"], MSByG,
                                      MSBySubCh, MSByPr)

ACTAsTS = cbind.data.frame("Date" = ClientName.pivot.table[,"Date"], ACTByG,
                                   ACTBySubCh, ACTByCSG, ACTByPr)

ACCAsTS = cbind.data.frame("Date" = ClientName.pivot.table[,"Date"], ACCbyG,
                                       ACCbySubCh, ACCbyCSG,
                                       ACCbyPr)

NationalAggregates = cbind.data.frame("Date" = ClientName.pivot.table[,"Date"],
                           "ACT" = ClientName.pivot.table[,"VACT"],
                           "MS" = ClientName.pivot.table[,"MS"],
                           "ACC" = ClientName.pivot.table[,"ACC"],
                           "IM" = ClientName.pivot.table[,"IM"])

#confirm that the values in each week's aggregates are accurate when compared to the original data
sum(st[which(st$G == "CityA" & st$Date == "xxxx-xx-xx"), "MS"])
MSAsTS$CityA.MS[which(MSAsTS$Date == "xxxx-xx-xx")]

###########################################################################################
#final data integration

#convert fb likes data into weeks
#read in the window of time that matches dataset
FixedClientNameFBData = ClientNameFBdata

#use alternative sentiment analysis results to compare with RSentiment results from above
FixedClientNameFBSent = read.csv("ClientName.facebook_sentiment.csv", header = TRUE, stringsAsFactors = FALSE)

#subset to get likes, comments, shares counts to aggregate
FixedFBvars = c("created_time","likes_count", "comments_count", "shares_count")
FixedClientNameFBData = FixedClientNameFBData[,FixedFBvars]

#convert fb time character data into posixlt posixt data objects
class(strptime(FixedClientNameFBData$created_time[1], format = "%Y-%m-%dT%H:%M:%S"))

FixedClientNameFBDatacreated_time = strptime(FixedClientNameFBData$created_time,
                                      format = "%Y-%m-%dT%H:%M:%S")
FixedClientNameFBSentdate = strptime(FixedClientNameFBSent$date,
                              format = "%m/%d/%Y")
#remove time vars for conversion to ts
FixedClientNameFBData = FixedClientNameFBData[,-1]
FixedClientNameFBSent = FixedClientNameFBSent[,-1]

#turn data into xts object ordered by date
FixedClientNameFBDataTS = xts(x = FixedClientNameFBData, order.by = FixedClientNameFBDatacreated_time)
FixedClientNameFBSentTS = xts(x = FixedClientNameFBSent, order.by = FixedClientNameFBSentdate)

#convert the xts into weekly data
FixedClientNameFBDataWeekly = apply.weekly(FixedClientNameFBDataTS, colSums)
FixedClientNameFBSentWeekly = apply.weekly(FixedClientNameFBSentTS, colSums)

#extract the aggregates to append them to the regressor dataset
extractFBMetrics = function(ListofData) {
  List1 = List2 = List3 = c()
  for (i in 1:nrow(ListofData)) {
    List1[i] = ListofData[i][[1]]
    List2[i] = ListofData[i][[2]]
    List3[i] = ListofData[i][[3]]
  }
  List = cbind(List1, List2, List3)
  return(List)
}

FixedWeekList = list()
FixedWeekList = extractFBMetrics(FixedClientNameFBDataWeekly)
colnames(FixedWeekList) = c("Likes", "Comments", "Shares")
FixedWeekList = as.data.frame(FixedWeekList)

extractSentiment = function(ListofData) {
  List1 = List2 = c()
  for (i in 1:nrow(ListofData)) {
    List1[i] = ListofData[i][[1]]
    List2[i] = ListofData[i][[2]]
  }
  List = cbind(List1, List2)
  return(List)
}

FixedSentList = list()
FixedSentList = extractSentiment(FixedClientNameFBSentWeekly)
colnames(FixedSentList) = c("NegativeSentimentComments", "PositiveSentimentComments")
FixedSentList = as.data.frame(FixedSentList)

#bind the FB counts and sentiment data together
ClientNameFBAggregates = cbind.data.frame(FixedWeekList, FixedSentList)

#bring in google trends data and sentiment data
ClientName.goog.trends = KeywordIndexes

#aggregate FB and goog data
AllSentimentData = cbind.data.frame(ClientNameFBAggregates,
                                    ClientName.goog.trends[2:length(ClientName.goog.trends)])

#bring in weekly unemployment claims
UnemplClaims = read.csv("WeeklyClaimsByState.csv", stringsAsFactors = FALSE)
#remove the high correlation variables from unemployment (Covered claims)
UnempRemove = grep("Covered", colnames(UnemplClaims))
UnemplClaims = UnemplClaims[,-which(names(UnemplClaims) %in% names(UnemplClaims)[UnempRemove])]

#bring in coincident index
Coincident = read.csv("Coincident Index (for TS).csv", header = TRUE)
colnames(Coincident) = sapply(colnames(Coincident), paste, ".coincident", sep = "")

#combine national aggregates with all acq,act,mkt,impr, fb, googtrends, unempl. coincident data
AllData = cbind.data.frame(NationalAggregates, ACTAsTS[,2:length(ACTAsTS)],
                           IMAsTS[,2:length(IMAsTS)],
                           MSAsTS[2:length(IMAsTS)],
                           ACCAsTS[,2:length(ACCAsTS)],
                           AllSentimentData,
                           UnemplClaims[,2:length(UnemplClaims)],
                           Coincident[,2:length(Coincident)])

AllData$Date = as.Date(AllData$Date, format = "%Y-%m-%d")

#blanks create a problem later, replace them
names(AllData) = gsub(" ", ".", names(AllData))

#vars that start with numbers also create a problem
colnames(AllData) = gsub("15\\.", "Fifteen.", colnames(AllData))
colnames(AllData) = gsub("25\\.", "TwentyFive.", colnames(AllData))
colnames(AllData) = gsub("30\\.", "Thirty.", colnames(AllData))
colnames(AllData) = gsub("75\\.", "SeventyFive.", colnames(AllData))
colnames(AllData) = gsub("2\\.", "Two.", colnames(AllData))
colnames(AllData) = gsub("-", "", colnames(AllData))

#some columns were read in as characters, convert to numerics
for (i in 1:length(AllData)) {
  if (class(AllData[,i]) != "numeric" & class(AllData[,i]) != "Date") {
    AllData[,i] = as.numeric(AllData[,i])
  }
}
#gsub alters data format, convert back to date
AllData$Date = as.Date(AllData$Date, format = "%Y-%m-%d")

#find unary variable with zeros, remove them
Summations = c()
for (i in 1:(length(AllData) - 1)) Summations[i] = sum(AllData[,i + 1])

Summations = c(105, Summations) #append the count of total dates

UnaryCount = grep("\\b0\\b", Summations) #zero only variables
colnames(AllData)[UnaryCount] #draw their names from column names

#remove unary columns from AllData
UnaryVariables = c("PSB.ACC", "PSNB.ACC",
                   "FR.ACC", "OD.ACC",
                   "PS.ACC", "SD.ACC")

#remove unary variables
AllData = AllData[,-which(names(AllData) %in% UnaryVariables)]

#end of data integration
########################################################################

#Begin EDA

#generate plots for all pivot table elements through productA keyword, write out to img files
AllPlots = list()
for (i in 1:100) {
  AllPlots = print(ggplot(data = as.data.frame(AllData[,i])) +
                          geom_line(aes(x = decimal_date(ymd(ClientName.pivot.table[,"Date"])),
                                        y = AllData[,i])) +
                          ggtitle(names(AllData)[i]) +
                          xlab("Observation Week") +
                          ylab(names(AllData)[i]))

  file_name = paste("AllPlots",i,".png",sep="")
  png(file_name)
  print(AllPlots)
  dev.off()
}

#can conversions/IM seasonality be normalized?
#rate of IM to conversion over time? on a per mille basis:
IRatioByWeek = 100*(AllData[,"ACT"] / (AllData[,"IM"]/1000))

IRatioByWeek = zoo(x = IRatioByWeek, order.by = AllData[,"Date"])

plot(x = decimal_date(ymd(AllData[,"Date"])), y = IRatioByWeek, type = "l",
     main = "VACT to 1000 IM Ratio By Week", xlab = " Decimal Date",
     ylab = "Ratio of ACT to PMIM")

#Append ACT to per mille IM to AllData
AllData$ACTPMIM = IRatioByWeek

#CPI by Ch?
#weekly
FindRatios = function(o,p) o / p
RemNaN = function(Ch) Ch[is.nan(Ch)] = 0

ICByChanODC = mapply(FindRatios, AllData$OD.C.MS,(AllData$OD.C.IM/1000))
ICByChanODC = RemNaN(ICByChanODC)

ICByChanODP = mapply(FindRatios, AllData$OD.Psp.MS,(AllData$OD.Psp.IM/1000))
ICByChanODP = RemNaN(ICByChanODP)

ICByChanPSB = mapply(FindRatios, AllData$PSB.MS,(AllData$PSB.IM/1000))
ICByChanPSB = RemNaN(ICByChanPSB)

ICByChanPSN = mapply(FindRatios, AllData$PSNB.MS,(AllData$PSNB.IM/1000))
ICByChanPSN = RemNaN(ICByChanPSN)

ICByChanPSO = mapply(FindRatios, AllData$PS.MS,(AllData$PS.IM/1000))
ICByChanPSO = RemNaN(ICByChanPSO)

ICByChanTVS = mapply(FindRatios, AllData$TV.MS,(AllData$TV.IM/1000))
ICByChanTVS = RemNaN(ICByChanTVS)

ICsByChanTS = cbind.data.frame("Date" = as.Date(ClientName.pivot.table[,"Date"]),
                                           "CPIByChan.Content" = ICByChanODC,
                                           "CPIByChan.Prsp" = ICByChanODP,
                                           "CPIByChan.BS" = ICByChanPSB,
                                           "CPIByChan.NOBS" = ICByChanPSN,
                                           "CPIByChan.PS" = ICByChanPSO,
                                           "CPIByChan.TV" = ICByChanTVS)

#append CPI by Ch to AllData
AllData = cbind.data.frame(AllData, ICsByChanTS[,2:length(ICsByChanTS)])

#graphically it looks like this
dev.new()
dev.off()
ggplot(ICsByChanTS, aes(x = Date, y = 'CPI', color = "CPIByChan.Content")) + geom_line() + ggtitle("CPM IM By SubCh") +
  geom_line(data = ICsByChanTS, aes(Date, CPIByChan.Prsp,
                                                color = "CPIByChan.Prsp")) +
  geom_line(data = ICsByChanTS, aes(Date, CPIByChan.BS,
                                                color = "CPIByChan.BS")) +
  geom_line(data = ICsByChanTS, aes(Date, CPIByChan.NOBS,
                                                color = "CPIByChan.NOBS")) +
  geom_line(data = ICsByChanTS, aes(Date, CPIByChan.PS,
                                                color = "CPIByChan.PS")) +
  geom_line(data = ICsByChanTS, aes(Date, CPIByChan.TV,
                                                color = "CPIByChan.TV")) +
  scale_color_manual(values = c("red", "navyblue", "green4","deeppink4","black","orange"))

#Prop of NL per week?
NLPropofAllACT = AllData$NL.ACT / AllData$ACT

#append this metric to AllData
AllData$NLProp.ACT = NLPropofAllACT

plot(x = AllData$Date, y = AllData$NLProp.ACT, type = "l",
     main = "NL As Prop of All ACT ", xlab = "Week of Observation",
     ylab = "Prop of All VACT")

ALPropofAllACT = AllData$AL.ACT / AllData$ACT

#append to AllData
AllData$ALProp.ACT = ALPropofAllACT

plot(x = AllData$Date, y = ALPropofAllACT, type = "l",
     main = "AL As Prop of All ACT ", xlab = "Week of Observation",
     ylab = "Prop of All VACT")


CanPropofAllACT = AllData$Can.V.ACT / AllData$ACT

#append this metric to AllData
AllData$CanVProp.ACT = CanPropofAllACT

plot(x = AllData$Date, y = CanPropofAllACT, type = "l",
     main = "Can V As Prop of All ACT ", xlab = "Week of Observation",
     ylab = "Prop of All VACT")

Growth = AllData$ALProp.ACT + AllData$Can.V.ACT.PropNatlACT
plot(x = AllData$Date, y = Growth, type = "l",
     main = "Can V + AL As Prop of All ACT ", xlab = "Week of Observation",
     ylab = "Prop of All VACT")

#Prop ACT from factors in re vars?
#list of factors in the re dataset + national aggregatate
GetNames = grep("ACT" , colnames(AllData), ignore.case = TRUE)

#reduce GetNames to only ACT, not Props
GetNames = GetNames[1:39]

#Prop ACT for each week per factor
ACTProps = function(GetNames) {
  Props = list()
  for (i in 1:length(GetNames)) {
    Props[[i]] = AllData[,GetNames[i]] / AllData$ACT
  }
  return(Props)
}

#generate a list of lists, length is full time series
Props = ACTProps(GetNames)

#name each list by source factor
names(Props) = names(AllData[,GetNames])

AppendPropDesignation = function(dfName) {
  for (i in 1:length(dfName)) {
    names(dfName)[i] = paste(names(dfName)[i], ".PropNatlACT", sep = "")
  }
  return(dfName)
}

Props = as.data.frame(AppendPropDesignation(Props))

#append to AllData
AllData = cbind.data.frame(AllData, Props)

#ACT plots for all factors to identify growth segments, save as png
ACTPlots = list()
for (i in 1:length(Props)) {
    ACTPlots[[i]] = print(ggplot(data = as.data.frame(Props[[i]])) +
      geom_line(aes(x = decimal_date(ymd(ClientName.pivot.table[,"Date"])),
                    y = Props[[i]])) +
      ggtitle(names(Props)[[i]]) +
      xlab("Observation Week") +
      ylab("Prop of Total ACT"))
    
    file_name = paste("ACTByFactorPlots", i, ".png", sep = "")
    png(file_name)
    print(ACTPlots[[i]])
    dev.off()    
}

#end EDA
########################################################################
#ARIMA Model
#PdS IM to PSB regression + lags

BuildDfForArima = function(df, n, p) {

  Target = df[,n]
  Regressor = df[,p]
  Lag1Regressor = lag(df[,p], 1)
  Lag1Regressor[1] = (Lag1Regressor[2]/Lag1Regressor[3]) / 2 #estimated value of lag 1
  Lag2Regressor = lag(df[,p], 2)
  Lag2Regressor[1] = (Lag2Regressor[3]/Lag2Regressor[4]) / 2 #estimated value of lag 1
  Lag2Regressor[2] = (Lag2Regressor[3]/Lag2Regressor[4]) / 2 #estimated value of lag 2
  ArimaModelVars = cbind.data.frame("re" = Target, "Regressor" = Regressor,
                                    "Lag1Regressor" = Lag1Regressor)#,
                                    #"Lag2Regressor" = Lag2Regressor)

  ArimaModel = arima(as.ts(Target[1:70]), order = c(1,1,1),
                     seasonal = list(order = c(0,1,1)),
                                     xreg = ArimaModelVars[1:70,2:length(ArimaModelVars)],
                                     optim.control = list(maxit = 1000), method = "ML")
}

NatlACTMktSp = list()
NatlACTMktSp = BuildDfForArima(AllData, 2, 3) #national ACT by MS
NatlACTMktSp$model
acf(NatlACTMktSp$residuals)

PDSocActPSB = list()
PDSocActPSB = BuildDfForArima(AllData, 32, 99) #PdS ACT-BSSp
pacf(PDSocActPSB$residuals)

ODContImpBrSrAct = list()
ODContImpBrSrAct = BuildDfForArima(AllData, 27, 67) #OD Content IM - BS ACT
pacf(ODContImpBrSrAct$residuals)
#end ARIMA
########################################################################
#Regression object components

#call, terms, residuals, coefficients[1:n] & t,p values, aliased, sigma, df, r.squared, adj.r.squared, fstatistic, cov.unscaled
#summary(A)$coefficients[1] # 1 = intercept estimate
#summary(A[[2]])$coefficients[2] # 2 = beta estimate for 1st regressor
# 3 = intercept stderror # 4 = beta est, 1st regressor stderror
# 5 = t value, intercept # 6 = t value, 1st regressor
# 7 = p value, intercept # 8 = p value, 1st regressor

################################################################################

#example of standard structural time series
fit = ts(AllData$IM, start = decimal_date(ymd("xxxx-xx-xx")), frequency = xx)
StdStrucTS = StructTS(fit, type = "BSM")
summary(StdStrucTS)
fit$coeff #gives the intercept, regressor coeffs for for each factor equation regressed on its lags
#also gives the var-covar matrices: variances and covariances
#parameters and unknowns: RHS coeffs, LHS coeffs,

#StructTS generates the following extractable values:
#coef, loglik, loglik0, data, residuals, fitted, call, series, code, model,
#model0, xtsp

IMExFFDOPr = as.numeric(ClientName.pivot.table$IM - FFDO)
par(mfrow = c(2,2))
plot(ClientName.pivot.table$IM, col = "blue", type = "o")
lines(FFDO$IM, col = "black")
lines(IMExFFDOPr, col = "red")

#correlation between IM and ACT
cor(ClientName.pivot.table$IM, ClientName.pivot.table$VACT)

################################################################

#################################################################
#arima and bsts with regressors

#note:your regressors don't have to be ts objects, they can be a simple matrix, but target must be

#it is recommended to use lagged re variable in regression if there is autocorrelation
LaggedreVars = matrix(0, nrow = nrow(AllData), ncol = length(GetNames))

for (i in 1:length(GetNames)) {
  LaggedreVars[,i] = lag(AllData[,GetNames[i]], 1)
}

for (i in 1:ncol(LaggedreVars)) {
  LaggedreVars[1,i] = sum(LaggedreVars[2,i],LaggedreVars[3,i]) / 2
}

colnames(LaggedreVars) = names(AllData[,GetNames])

#aggregate variables in AllData you want to use as regressors
Xregressors = AllData[,-which(names(AllData) %in% names(AllData[,GetNames]))]

#subset to exclude holdout data
XregressorsSub = Xregressors[1:(nrow(AllData) - 15),]

#loop through all possible regressors to see which are best modeled
ACTArima = list()
for (i in 1:length(GetNames)) {
  #convert ACT to ts, use 316 regressors excluding other ACT but incl. its lag
  #order(2,1,4) is a better model, but will not converge when using lagged.ACT
  ACTArima[[i]] = arima(as.ts(AllData[1:(nrow(AllData) - 15),names(AllData[,GetNames[i]])]),
                               order = c(1,1,1), seasonal = list(order = c(0,1,1),
                                                                 xreg = cbind(XregressorsSub,
                                                                 LaggedreVars[1:90,i]),
                                                         optim.control = list(maxit = 10000)))
  }
names(ACTArima) = names(AllData[,GetNames])

#look at the MA and AR components to ID outliers or seasonality
ACFPlots = list()
PACFPlots = list()
for (i in 1:length(ACTArima)) {
  ACFPlots[[i]] = acf(ACTArima[[i]]$residuals)
  PACFPlots[[i]] = pacf(ACTArima[[i]]$residuals)
}

#extract acf values at different lags, find the lowest acf ACT regression (best predicted ACT factor)
AllACFs = matrix(0, nrow = 20, ncol = 39)
for (k in 1:39) {
  for (i in 1:20) {
    AllACFs[i,k] = ACFPlots[[k]]$acf[i]
  }
}

AllPACFs = matrix(0, nrow = 20, ncol = 39)
for (k in 1:39) {
  for (i in 1:20) {
    AllPACFs[i,k] = PACFPlots[[k]]$acf[i]
  }
}

AllPACFs = AllPACFs[1:19,]
MeanACTACF = colMeans(AllACFs)
MeanACTPACF = colMeans(AllPACFs)
ACFsAndPACFs = cbind.data.frame(names(AllData[,GetNames]), "Mean ACF ACT Type" = MeanACTACF,
                                "Mean PACF ACT Type" = MeanACTPACF)
write.csv(ACFsAndPACFs, "ACFsPACFs.csv")
ggplot(data = ACFsAndPACFs, aes(x = names(AllData[,GetNames]), y = MeanACTACF)) +
  geom_bar(stat = "identity", position = "identity") +
  theme(axis.text.x = element_text(angle = -90, hjust = 0)) +
  xlab("") + ylab("") + ggtitle("Mean ACF by ACT Type")

ggplot(data = ACFsAndPACFs, aes(x = names(AllData[,GetNames]), y = MeanACTPACF)) +
  geom_bar(stat = "identity", position = "identity") +
  theme(axis.text.x = element_text(angle = -90, hjust = 0)) +
  xlab("") + ylab("") + ggtitle("Mean PACF by ACT Type")

#convert pivot table date to Date class
Date = ymd(as.character(AllData$Date))

#build a dataframe of fitted and predicted values by date, predicted 15 observations out
ActvsPred = list()
for (i in 1:length(ACTArima)) {
ActvsPred[[i]] = data.frame(c(as.numeric(fitted(ACTArima[[i]])),
                         as.numeric(predict(ACTArima[[i]], n.ahead = 15)$pred)),
                       as.numeric(AllData[,GetNames[i]]),
                       Date)
}

#name the data frame columns
for (i in 1:length(ActvsPred)) {
  names(ActvsPred[[i]]) = c("Fitted", "Actual", "Date")
}

#mean MAPE
MeanMAPEs = list()
for (i in 1:length(ActvsPred)) {
  MeanMAPEs[[i]] = mean(abs(ActvsPred[[i]]$Actual - ActvsPred[[i]]$Fitted) / ActvsPred[[i]]$Actual)
}
MeanMAPEs

#MAPE for the predicted values
ArimaPredictedMAPE = list()
for (i in 1:length(ActvsPred)) {
  ArimaPredictedMAPE[[i]] = filter(ActvsPred[[i]], Date > ymd("xxxx-xx-xx")) %>%
    summarise(ArimaPredictedMAPE = mean(abs(Actual - Fitted) / Actual))
}

#Lowest forecast MAPEs
names(AllData[,GetNames])[3] #CityA.ACT
names(AllData[,GetNames])[4] #CityF.ACT
names(AllData[,GetNames])[11] #CityM.ACT
names(AllData[,GetNames])[12] #CityR.ACT
names(AllData[,GetNames])[24] #FactorB.ACT
names(AllData[,GetNames])[29] #FactorD.ACT
names(AllData[,GetNames])[31] #Can V.ACT

#rename to fit into main titles in graphs
AllTargetVars2 = names(AllData[,GetNames])

#plot the actual and fit data for all actual and predicted data across all types of ACT
#label the graphs x,y axes and main title
ArimaPlots = list()
for (i in 1:length(AllTargetVars2)) {
  ArimaPlots[[i]] = print(ggplot(data = ActvsPred[[i]], aes(x = Date)) +
                          geom_line(aes(y = Actual, colour = "Actual"), size = 1.2) +
                          geom_line(aes(y = Fitted, colour = "Fitted"), size = 1.2, linetype = 2) +
                          ggtitle(paste0(AllTargetVars2[i],
                                         " ARIMA Predicted MAPE Last 15 Obs = ",
                                         round(100 * ArimaPredictedMAPE[[i]], 2), "%")) +
                          theme(axis.text.x = element_text(angle = -90, hjust = 0)))
}

#write them out to image files
for (i in 1:length(AllTargetVars2)) {
  file_name = paste("ArimaPlots", i, ".png", sep = "")
  png(file_name)
  print(ArimaPlots[[i]])
  dev.off()
}


#end of ARIMA models
########################################################################

ACTvsMktSpl = AllData[,c(1:3,98,101,102)]

ACTvsMktSpl$NonProsp = ACTvsMktSpl$MS - ACTvsMktSpl$OD.Psp.MS

ACTvsMktSpl$NonTv = ACTvsMktSpl$MS - ACTvsMktSpl$TV.MS

ACTvsMktSpl$NonPdSoc = ACTvsMktSpl$MS - ACTvsMktSpl$PS.MS

ACTvsMktSpl$NonTvPdSocProsp = ACTvsMktSpl$MS - ACTvsMktSpl$PS.MS - ACTvsMktSpl$TV.MS - ACTvsMktSpl$OD.Psp.MS

cor(ACTvsMktSpl$ACT,ACTvsMktSpl$NonTvPdSocProsp) #.5347 correlation!!

#ACTvsMktSpl = ACTvsMktSpl[order(ACTvsMktSpl$ACT),]
fit1 = lm(ACT~NonTvPdSocProsp, data = ACTvsMktSpl)
fit1 = glm(ACT~NonTvPdSoc, data = ACTvsMktSpl, family = "quasipoisson")
#quasipoisson addressed the over dispersion of residuals (non-normal error distribution)
#1 coefficients 
#2 residuals
#3 effects 
#4 rank
#5 fitted.values 
#6 assign
#7 qr 
#8 df.residual
#9 xlevels 
#10 call
#11 terms 
#12 model
options(scipen = 9999999, "digits" = 8)

fit1$coefficients[2]
plot(fit1$residuals, main = "Residuals-ACT (Lowest - Highest)")
#note how, as ACT increase, the unexplained portion of those ACT increases
hist(fit1$residuals)
summary(fit1)

fit2 = lm(ACT~NonTvPdSocProsp + PS.MS + OD.Psp.MS, data = ACTvsMktSpl)
summary(fit2)
plot(fit2$residuals, main = "Residuals-ACT on MS")

Baseline = cbind.data.frame(ACTvsMktSpl$ACT, fit2$fitted.values)
Baseline$Difference = as.integer(Baseline[,1] - Baseline[,2])
Baseline$TVMSP = ACTvsMktSpl$TV.MS
Baseline$Prsp = ACTvsMktSpl$OD.Psp.MS
fit3 = lm(Difference ~ Prsp, data = Baseline)
summary(fit3)
plot(fit3$residuals, main = "Residuals-ACT (Lowest - Highest)")

#PerformanceAnalytics package
#use to build a correlation chart
#chart.Correlation(AllData,
#                  method="spearman",
#                  histogram=TRUE,
#                  pch=16)



#psych package
#corr.test(Data.num,
#          use = "pairwise",
#          method="spearman",
#          adjust="none",      # Can adjust p-values; see ?p.adjust for options
#          alpha=.05)

########################################################################
#MS is negatively correlated with PR3
Correlations = c()
for(i in 1:length(GetNames)) {
  Correlations[i] = cor(AllData$PS.MS, AllData[,GetNames[i]])
}

names(AllData[,GetNames])[13]
plot(Correlations)
Correlations
Correlations = cor(AllData[,2:407])
write.csv(Correlations, "ClientNamePivotCorrelations.csv")

########################################################################

#beginning of single data frame bsts model as built by Kim Laarsens @
#https://kim...

AllTargetVars = AllTargetVars2
#cbind re variables from AllTargetVars into data frame with regressors
#differs from AllData -> Xregressors regression in that it's not dynamically calling AllData
#run regression on one type of ACT factor at a time
#note that the dates were removed to facilitate regression [,2:length...]

ACTPlusRegressors = list()
for (i in 1:length(AllTargetVars)) {
  ACTPlusRegressors[[i]] = cbind.data.frame(AllData[,AllTargetVars[i]],
                                         Xregressors[,2:length(Xregressors)])
}

#name the 1st column of ACTPlusRegressors w/ corresponding target var title
for (i in 1:length(AllTargetVars)) {
  names(ACTPlusRegressors[[i]])[1] = AllTargetVars[i]
}

#run batch regression
TwoFor = as.data.frame(ACTPlusRegressors[[18]])

post.period = c(91,105)
post.period.re = TwoFor[post.period[1]:post.period[2],1]

#turn values to be predicted to NAs
TwoFor[post.period[1]:post.period[2],1] = NA

#semi local linear trend assumes slope component is AR(1) instead of random walk, using LU keyword search lagged by 1 week
LagLU = lag(Xregressors[1:105,112], 1)
LagLU[is.na(LagLU)] = Xregressors[1,112]

ssR = AddSemilocalLinearTrend(list(), LagLU)

#presume each year is a full season
ssR = AddSeasonal(ssR, TwoFor[,1], nseasons = 2, season.duration =  52)

reVar = TwoFor[,1]

bsts.model = bsts(reVar ~ ., state.specification = ssR, niter = 10,
                   ping = 1, seed = 2014, data = TwoFor[,2:length(TwoFor)])

#now deconstruct the model to see trend and regression
plot(bsts.model,"components")

#predict holdout data using a burn in period
burn = SuggestBurn(0.1, bsts.model)

PositiveMean = function(b) {
  b = b[abs(b) > 0]
  if (length(b) > 0)
    return(mean(b))
  return(0)
}

options(scipen = 9999999, "digits" = 8)

# Get the average coefficients when variables were selected (non-zero slopes)
coeff = data.frame(melt(apply(bsts.model$coefficients[-(1:burn),], 2, PositiveMean)))
coeff$Variable = as.character(row.names(coeff))
coeff = coeff[abs(coeff$value) > .0001,1:2]

BSTSSignifVarPlot = list()

BSTSSignifVarPlot = ggplot(data = coeff, aes(x = Variable, y = value)) +
  geom_bar(stat = "identity", position = "identity") +
  theme(axis.text.x = element_text(angle = -90, hjust = 0)) +
  xlab("") + ylab("") + ggtitle("Average coefficients")

#write them out to image files
for (i in 1) {
  file_name = paste("BSTSSignifVarPlot", i, ".png", sep = "")
  png(file_name)
  print(BSTSSignifVarPlot)
  dev.off()
}

#predict using MS for the holdout data, i.e. conditional on mkt.sp -> y values
bsts.predicted = predict.bsts(bsts.model, horizon = 15, burn = burn, quantiles = c(.025, .975),
                               newdata = TwoFor[91:105,])

#put fitted+predicted and actual in dataframe for comparison, note that since the NAs were
#predicted during modeling, the colmeans are 105 values long, replace these with bsts.predicted.mean
#use data from pivot table used in ARIMA
BSTSActvsPred = data.frame(
  c(as.numeric(-colMeans(bsts.model$one.step.prediction.errors[-(1:burn),])[1:90] +
                 TwoFor[1:90,1]), as.numeric(bsts.predicted$mean)),
  c(as.numeric(TwoFor[1:90,1]),post.period.re), Date)

names(BSTSActvsPred) <- c("Fitted", "Actual", "Date")

#MAPE using ACT regressed on MS w/ state spec PR1 mkt
BSTSMAPE = filter(BSTSActvsPred, Date > decimal_date(as.POSIXlt(ymd("xxxx-xx-x")))) %>% summarise("BSTSMAPE" = mean(abs(Actual - Fitted) / Actual))

#use values form above to determine causal impact (p.p.r. - actual data)
impact = CausalImpact(bsts.model = bsts.model,
                       post.period.re = post.period.re)

#get the 3 charts showing counterfactual, prediction, and pointwise CE (difference from counterfact)
#and the cumulative effect
plot(impact)
summary(impact)
summary(impact, "report")


#citation: CausalImpact 1.1.3, Brodersen et al., Annals of Applied Statistics (2015). http://google.github.io/CausalImpact/”
#################################################################################
#run plsr (PCA + regression) on each dataframe

VarImp = plsr(ACT ~ ., data = AllData)

save = as.matrix(varImp(VarImp))

AllData

###################################################################################
#batch bsts model using ACTPlusRegressors[[x]] as input data frames

BatchBSTS = function(df) {

  df = as.data.frame(df)

  Imputation = function(column, zeros) {
    df[which(df[,column] == 0), column] = round(abs(rnorm(zeros, mean = 0, sd = 1)))
    return(df[,column])
  }

  zeros = c(); for (i in 1:ncol(df)) zeros[i] = length(which(df[,i] == 0))

  for (i in 1:ncol(df)) {
    k = zeros[i]
    df[,i] = Imputation(i,k)
  }

  post.period = c(53,105)
  post.period.re = df[post.period[1]:post.period[2],1]

  df[post.period[1]:post.period[2],1] = NA

  #semi local linear trend assumes slope component is AR(1) instead of random walk, using LU keyword search lagged by 1 week
  LagLU = lag(Xregressors[1:105,112], 1)
  LagLU[is.na(LagLU)] = Xregressors[1,112] # replace lagged NA to keep obs. 1 for other vars; not signif. impact on model

  ssR = AddSemilocalLinearTrend(list(), LagLU) # empty list is part of syntax

  #4 seasons per year
  ssR = AddSeasonal(ssR, df[,1], nseasons = 8, season.duration =  13)

  df$reVar = df[,1]
  actName = colnames(df)[1]
  df = df[, -1]

  bsts.model = bsts(reVar ~ ., state.specification = ssR, niter = 150,
                     ping = 50, seed = 2014, data = df)

  #predict holdout data using a burn in period
  burn = SuggestBurn(0.1, bsts.model)

  PositiveMean = function(b) {
    b = b[abs(b) > 0]
    if (length(b) > 0)
      return(mean(b))
    else {return(0)}
  }

  # turn off scientific notation
  options(scipen = 9999999, "digits" = 7)

  ### Get the average coefficients when variables were selected (non-zero slopes)
  coeff = data.frame(melt(apply(bsts.model$coefficients[-(1:burn),], 2,
                                 PositiveMean)))
  coeff$Variable = as.character(row.names(coeff))
  coeff = coeff[abs(coeff$value) > .0001,1:2]

  BSTSSignifVarPlot = list()
  BSTSSignifVarPlot = ggplot(data = coeff, aes(x = Variable, y = value)) +
    geom_bar(stat = "identity", position = "identity") +
    theme(axis.text.x = element_text(angle = -90, hjust = 0)) +
    xlab("") + ylab("") + ggtitle("Average coefficients")

  #Note that horizon has to be changed when changing post.period values
  bsts.predicted = predict.bsts(bsts.model, horizon = 53, burn = burn,
                                 quantiles = c(.025, .975),
                                 newdata = df[53:105,])

  #put fitted+predicted and actual in dataframe for comparison, note that since the NAs were
  #predicted during modeling, the colmeans must be 105 values long (cbind fitted values plus error and mean of predicted values for all iteration), replace these with bsts.predicted.mean
  #use Date from pivot table used for ARIMA
  BSTSActvsPred = data.frame(
    c(as.numeric(-colMeans(bsts.model$one.step.prediction.errors[-(1:burn),])[1:52] +
                   df[1:52,"reVar"]), as.numeric(bsts.predicted$mean)),
    c(as.numeric(df[1:52,"reVar"]),post.period.re), Date)

  names(BSTSActvsPred) = c("Fitted", "Actual", "Date")

  # MAPE
  BSTSMAPE = filter(BSTSActvsPred, Date > decimal_date(as.POSIXlt(ymd("xxxx-xx-xx")))) %>% summarise("BSTSMAPE" = mean(abs(Actual - Fitted) / Actual))

  #use values form above to determine causal impact (p.p.r. - actual data)
  impact = CausalImpact(bsts.model = bsts.model,
                         post.period.re = post.period.re)

  list(bsts.model, BSTSSignifVarPlot, BSTSActvsPred, BSTSMAPE, impact, actName)

}

AllACTBSTS = list()
for (i in 1:length(AllTargetVars)) {
  print(i)
  AllACTBSTS[[i]] = BatchBSTS(ACTPlusRegressors[[i]])
}

names(ACTPlusRegressors)[13]

AllACTBSTS[[23]][5][1]
# Index | Content
#     1 | bsts.model
#     2 | BSTS SignifVarPlot
#     3 | BSTS Pred
#     4 | BSTS MAPE
#     5 | impact analysis
#   5.1 | impact series
#   5.2 | impact summary
#   5.3 | impact report
#   5.4 | impact model
#     6 | ACT type


################################################################################

BatchBSTS = function(df) {

  df = as.data.frame(df)

  Imputation = function(column, zeros) {
    df[which(df[,column] == 0), column] = round(abs(rnorm(zeros, mean = 0, sd = 1)))
    return(df[,column])
  }

  zeros = c(); for (i in 1:ncol(df)) zeros[i] = length(which(df[,i] == 0))

  for (i in 1:ncol(df)) {
    k = zeros[i]
    df[,i] = Imputation(i,k)
  }

  post.period = c(53,105)
  post.period.re = df[post.period[1]:post.period[2],1]

  df[post.period[1]:post.period[2],1] = NA

  #semi local linear trend assumes slope component is AR(1) instead of random walk, using LU keyword search lagged by 1 week
  LagLU = lag(Xregressors[1:105,112], 1)
  LagLU[is.na(LagLU)] = Xregressors[1,112] # replace lagged NA to keep obs. 1 for other vars; not signif. impact on model

  ssR = AddSemilocalLinearTrend(list(), LagLU) # empty list is part of syntax

  #4 seasons per year
  ssR = AddSeasonal(ssR, df[,1], nseasons = 8, season.duration =  13)

  df$reVar = df[,1]
  actName = colnames(df)[1]
  df = df[, -1]

  bsts.model = bsts(reVar ~ ., state.specification = ssR, niter = 150,
                     ping = 50, seed = 2014, data = df)

  #predict holdout data using a burn in period
  burn = SuggestBurn(0.1, bsts.model)

  PositiveMean = function(b) {
    b = b[abs(b) > 0]
    if (length(b) > 0)
      return(mean(b))
    else {return(0)}
  }

  options(scipen = 9999999, "digits" = 7)

  coeff = data.frame(melt(apply(bsts.model$coefficients[-(1:burn),], 2,
                                 PositiveMean)))
  coeff$Variable = as.character(row.names(coeff))
  coeff = coeff[abs(coeff$value) > .0001,1:2]

  BSTSSignifVarPlot = list()
  BSTSSignifVarPlot = ggplot(data = coeff, aes(x = Variable, y = value)) +
    geom_bar(stat = "identity", position = "identity") +
    theme(axis.text.x = element_text(angle = -90, hjust = 0)) +
    xlab("") + ylab("") + ggtitle("Average coefficients")

  bsts.predicted = predict.bsts(bsts.model, horizon = 53, burn = burn,
                                 quantiles = c(.025, .975),
                                 newdata = df[53:105,])

  BSTSActvsPred = data.frame(
    c(as.numeric(-colMeans(bsts.model$one.step.prediction.errors[-(1:burn),])[1:52] +
                   df[1:52,"reVar"]), as.numeric(bsts.predicted$mean)),
    c(as.numeric(df[1:52,"reVar"]),post.period.re), Date)

  names(BSTSActvsPred) = c("Fitted", "Actual", "Date")

  BSTSMAPE = filter(BSTSActvsPred, Date > decimal_date(as.POSIXlt(ymd("xxxx-xx-xx")))) %>% summarise("BSTSMAPE" = mean(abs(Actual - Fitted) / Actual))

  impact = CausalImpact(bsts.model = bsts.model,
                         post.period.re = post.period.re)

  list(bsts.model, BSTSSignifVarPlot, BSTSActvsPred, BSTSMAPE, impact, actName)

}

AllACTBSTS = list()
for (i in 1:length(AllTargetVars)) {
  print(i)
  AllACTBSTS[[i]] = BatchBSTS(ACTPlusRegressors[[i]])
}

